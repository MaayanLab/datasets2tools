{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# %run /Users/denis/Documents/Projects/scripts/init.ipy\n",
    "%run /Users/maayanlab/Documents/init.ipy\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine, MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session\n",
    "engine = create_engine('mysql://root:MyNewPass@localhost/datasets2tools')\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4702L]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query({'tool_name': 'Enrichr'}, 'tool', session, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_query(query, object_type, session, metadata):\n",
    "\n",
    "\t# Dataset\n",
    "\tif object_type == 'dataset':\n",
    "\n",
    "\t\t# By accession\n",
    "\t\tif 'dataset_accession' in query.keys():\n",
    "\t\t\tquery = session.query(metadata.tables[object_type].columns['id']).filter(metadata.tables[object_type].columns['tool_name']==query['dataset_accession'])\n",
    "\n",
    "\t# Tool\n",
    "\telif object_type == 'tool':\n",
    "\n",
    "\t\t# By name\n",
    "\t\tif 'tool_name' in query.keys():\n",
    "\t\t\tquery = session.query(metadata.tables[object_type].columns['id']).filter(metadata.tables[object_type].columns['tool_name']==query['tool_name'])\n",
    "\n",
    "\t# Get ID list\n",
    "\tids = [x[0] for x in query.all()]\n",
    "\n",
    "\t# Return ids\n",
    "\treturn ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'articles': [{'abstract': '{\"abstract\": [[\"Motivation:\", \"In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inflexibility. However, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and RandomForest (RF) models. Recently, it has been observed that RF models are biased in such a way that categorical variables with a large number of categories are preferred.\"], [\"Results:\", \"In this work, we introduce a heuristic for normalizing feature importance measures that can correct the feature importance bias. The method is based on repeated permutations of the outcome vector for estimating the distribution of measured importance for each variable in a non-informative setting. The P-value of the observed importance provides a corrected measure of feature importance. We apply our method to simulated data and demonstrate that (i) non-informative predictors do not receive significant P-values, (ii) informative variables can successfully be recovered among non-informative variables and (iii) P-values computed with permutation importance (PIMP) are very helpful for deciding the significance of variables, and therefore improve model interpretability. Furthermore, PIMP was used to correct RF-based importance measures for two real-world case studies. We propose an improved RF model that uses the significant variables with respect to the PIMP measure and show that its prediction accuracy is superior to that of other existing models.\"], [\"Availability:\", \"R code for the method presented in this article is available at http://www.mpi-inf.mpg.de/\\\\u223caltmann/download/PIMP.R\"], [\"Contact:\", \"altmann@mpi-inf.mpg.de, laura.tolosi@mpi-inf.mpg.de\"], [\"Supplementary information:\", \"Supplementary data are available at Bioinformatics online.\"]]}',\n",
       "   'article_title': 'Permutation importance: a corrected feature importance measure',\n",
       "   'authors': 'Andr\\xc3\\xa9 Altmann; Laura Tolo\\xc5\\x9fi; Oliver Sander; Thomas Lengauer',\n",
       "   'date': datetime.date(2010, 4, 12),\n",
       "   'doi': 'https://doi.org/10.1093/bioinformatics/btq134',\n",
       "   'id': 1L,\n",
       "   'journal_fk': None,\n",
       "   'tool_fk': 1L}],\n",
       " 'id': 1L,\n",
       " 'tool_description': 'A corrected feature importance measure',\n",
       " 'tool_homepage_url': 'http://www.mpi-inf.mpg.de/\\xe2\\x88\\xbcaltmann/download/PIMP.R',\n",
       " 'tool_icon_url': None,\n",
       " 'tool_name': 'Permutation importance'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_object_data(1, 'tool', session, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_type='tool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table('related_tool', MetaData(bind=None), Column('id', INTEGER(display_width=11), table=<related_tool>, primary_key=True, nullable=False), Column('source_tool_fk', INTEGER(display_width=11), ForeignKey(u'tool.id'), table=<related_tool>), Column('target_tool_fk', INTEGER(display_width=11), ForeignKey(u'tool.id'), table=<related_tool>), Column('similarity', DOUBLE(asdecimal=True), table=<related_tool>), schema=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get related objects\n",
    "session.query(metadata.tables['related_'+object_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_object_data(object_id, object_type, session, metadata):\n",
    "\n",
    "\t# Dataset\n",
    "\tif object_type == 'dataset':\n",
    "\t\tpass\n",
    "\n",
    "\telif object_type == 'tool':\n",
    "\t\t\n",
    "\t\t# Perform tool query\n",
    "\t\ttool_query = session.query(metadata.tables[object_type]).filter(metadata.tables[object_type].columns['id'] == object_id)\n",
    "\n",
    "\t\t# Get tool data\n",
    "\t\tobject_data = {key: value for key, value in zip(metadata.tables[object_type].columns.keys(), tool_query.all()[0])}\n",
    "\n",
    "\t\t# Perform article query\n",
    "\t\tarticle_query = session.query(metadata.tables['article']).filter(metadata.tables['article'].columns['tool_fk'] == object_id)\n",
    "\n",
    "\t\t# Get article data\n",
    "\t\tobject_data['articles'] = [{key: value for key, value in zip(metadata.tables['article'].columns.keys(), query_result)} for query_result in article_query.all()]\n",
    "\n",
    "\telif object_type == 'canned_analysis':\n",
    "\t\tpass\n",
    "\n",
    "\t# Return\n",
    "\treturn object_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id=1\n",
    "object_type='tool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_query = session.query(metadata.tables[object_type]).filter(metadata.tables[object_type].columns['id'] == object_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_data = {key: value for key, value in zip(metadata.tables[object_type].columns.keys(), tool_query.all()[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_query = session.query(metadata.tables['article']).filter(metadata.tables['article'].columns['tool_fk'] == object_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = article_query.all()\n",
    "article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = session.query(metadata.tables['dataset'], metadata.tables['repository'])\n",
    "a.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = engine.execute(table.select('id')).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = session.query(metadata.tables['article'])\n",
    "a.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = session.query(table.columns['id']).filter(table.columns['tool_name']=='Enrichr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [x[0] for x in results.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = session.query(table).filter(table.columns['id']==ids[0])#.all()\n",
    "results.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "{key: value for key, value in zip(table.columns.keys(), results.all()[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(table.select().where(table.columns['tool_name'] == 'genemania')).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(set([keys for metadata_dict in canned_analysis_dataframe['metadata'] for keys in metadata_dict.keys() for term_name in keys]))).rename('term_name').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_object_data(1, 'tool', session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_object_data(object_id, object_type, session, ):\n",
    "\n",
    "\t# Dataset\n",
    "\tif object_type == 'dataset':\n",
    "\t\tpass\n",
    "\n",
    "\telif object_type == 'tool':\n",
    "\t\t\n",
    "\t\t# Perform tool query\n",
    "\t\ttool_query = session.query(metadata.tables[object_type]).filter(metadata.tables[object_type].columns['id'] == object_id)\n",
    "\n",
    "\t\t# Get tool data\n",
    "\t\tobject_data = {key: value for key, value in zip(metadata.tables[object_type].columns.keys(), tool_query.all()[0])}\n",
    "\n",
    "\t\t# Perform article query\n",
    "\t\tarticle_query = session.query(metadata.tables['article']).filter(metadata.tables['article'].columns['tool_fk'] == object_id)\n",
    "\n",
    "\t\t# Get article data\n",
    "\t\tobject_data['articles'] = [{key: value for key, value in zip(metadata.tables['article'].columns.keys(), query_result)} for query_result in article_query.all()]\n",
    "\n",
    "\telif object_type == 'canned_analysis':\n",
    "\t\tpass\n",
    "\n",
    "\t# Return\n",
    "\treturn object_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "canned_analysis_dataframe = pd.read_table(infile)\n",
    "\n",
    "# Load metadata JSON\n",
    "canned_analysis_dataframe['metadata'] = [json.loads(x) for x in canned_analysis_dataframe['metadata']]\n",
    "\n",
    "# Get dataset, tool and canned analysis dataframes to upload\n",
    "dataframes_to_upload = {\n",
    "    'dataset': canned_analysis_dataframe['dataset_accession'].to_frame().drop_duplicates(),\n",
    "    'tool': canned_analysis_dataframe['tool_name'].to_frame().drop_duplicates(),\n",
    "    'canned_analysis': canned_analysis_dataframe.drop(['dataset_accession', 'tool_name', 'metadata'], axis=1),\n",
    "    'term': pd.Series(list(set([term_name for metadata_dict in canned_analysis_dataframe['metadata'] for term_name in keys for term_name in metadata_dict.keys()]))).rename('term_name').to_frame()\n",
    "}\n",
    "\n",
    "# Upload dataframes and get IDs\n",
    "id_data = {object_type: upload_and_get_ids(dataframe_to_upload, object_type, engine) for object_type, dataframe_to_upload in dataframes_to_upload.iteritems()}\n",
    "\n",
    "# Add foreign keys\n",
    "fk_conversion_dataframe = canned_analysis_dataframe.merge(id_data['canned_analysis'], on='canned_analysis_url', how='left').merge(id_data['tool'], on='tool_name', how='left').merge(id_data['dataset'], on='dataset_accession', how='left')[['dataset_fk', 'tool_fk', 'canned_analysis_fk', 'metadata']]\n",
    "\n",
    "# Upload dataset and tool matching\n",
    "for object_type in ['dataset', 'tool']:\n",
    "    \n",
    "    # Get table object\n",
    "    table = Table(object_type, MetaData(), autoload=True, autoload_with=engine)\n",
    "    \n",
    "    # Upload\n",
    "    engine.execute(table.insert().prefix_with('IGNORE'), fk_conversion_dataframe[['canned_analysis_fk', object_type+'_fk']].to_dict(orient='records'))\n",
    "\n",
    "# Initialize metadata dataframe\n",
    "metadata_dataframe_ready_to_upload = pd.DataFrame()\n",
    "\n",
    "# Loop through canned analysis dataframe\n",
    "for index, rowData in fk_conversion_dataframe.iterrows():\n",
    "    \n",
    "    # Get metadata dataframe\n",
    "    metadata_dataframe = pd.Series(rowData['metadata']).to_frame().reset_index().rename(columns={'index': 'term_name', 0: 'value'}).merge(id_data['term'], on='term_name', how='left').drop('term_name', axis=1)\n",
    "\n",
    "    # Add canned analysis foreign key\n",
    "    metadata_dataframe['canned_analysis_fk'] = rowData['canned_analysis_fk']\n",
    "    \n",
    "    # Concantenate\n",
    "    metadata_dataframe_ready_to_upload = pd.concat([metadata_dataframe_ready_to_upload, metadata_dataframe])\n",
    "\n",
    "# Get table object\n",
    "canned_analysis_metadata = Table('canned_analysis_metadata', MetaData(), autoload=True, autoload_with=engine)\n",
    "\n",
    "# Upload\n",
    "engine.execute(canned_analysis_metadata.insert().prefix_with('IGNORE'), metadata_dataframe_ready_to_upload.to_dict(orient='records'))\n",
    "\n",
    "# Return\n",
    "results = json.dumps(canned_analysis_dataframe.merge(id_data['canned_analysis'], on='canned_analysis_url').rename(columns={'canned_analysis_fk': 'id'}).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_and_get_ids(dataframe_to_upload, table_name, engine, identifiers={'tool': 'tool_name', 'dataset': 'dataset_accession', 'canned_analysis': 'canned_analysis_url', 'article': 'doi', 'term': 'term_name'}):\n",
    "\n",
    "    # Get table object\n",
    "    table = Table(table_name, MetaData(), autoload=True, autoload_with=engine)\n",
    "\n",
    "    # Insert data\n",
    "    engine.execute(table.insert().prefix_with('IGNORE'), dataframe_to_upload.to_dict(orient='records'))\n",
    "\n",
    "    # Get data\n",
    "    table_data = engine.execute(table.select())\n",
    "\n",
    "    # Get identifier column\n",
    "    identifier_column = identifiers[table_name]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    result_dataframe = pd.DataFrame(table_data.fetchall(), columns=table_data.keys())[['id', identifier_column]]\n",
    "\n",
    "    # Merge IDs\n",
    "    id_dataframe = dataframe_to_upload.merge(result_dataframe, on=identifier_column, how='left')[['id', identifier_column]].rename(columns={'id': table_name+'_fk'})\n",
    "\n",
    "    # Return\n",
    "    return id_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
